# Полносвязная нейронная сеть для классификации изображений


Задание 1: Изменение архитектуры сети
Измените количество нейронов в скрытом слое (например, 64, 256, 512) и количество скрытых слоев (добавьте еще один или два слоя). Обучите модель с измененной архитектурой и сравните результаты с исходной моделью. Как изменение количества нейронов и слоев влияет на точность и скорость обучения? Результаты задокументировать в таблице.

Задание 2: Эксперименты с функциями активации
Замените функцию активации relu в скрытом слое на другие функции, такие как sigmoid, tanh или elu. Обучите модель с каждой из этих функций и сравните результаты. Какие функции активации дают лучшую точность? Влияют ли разные функции активации на скорость обучения? Результаты задокументировать в таблице.

Задание 3: Оптимизаторы
Попробуйте использовать различные оптимизаторы, такие как SGD, RMSprop или Adagrad, вместо adam. Настройте параметры оптимизаторов (например, learning rate для SGD). Сравните результаты и объясните влияние различных оптимизаторов на процесс обучения и итоговую точность. Результаты задокументировать в таблице.

Задание 4: Оценка качества обучения и борьба с переобучением
⦁ Добавление dropout: Добавьте слой Dropout после скрытого слоя (или слоев) с коэффициентом dropout от 0.2 до 0.5. Dropout - это метод регуляризации, который случайным образом "выключает" нейроны во время обучения, что помогает предотвратить переобучение.
⦁ Изменение размера батча: Экспериментируйте с разными размерами батча (например, 16, 64, 128, 256). Как размер батча влияет на скорость обучения и точность?
⦁ Early stopping: Используйте callback EarlyStopping для остановки обучения, когда модель перестает улучшаться на валидационном наборе данных. Это поможет предотвратить переобучение. Подсказка: используйте EarlyStopping(monitor='val_loss', patience=3) для отслеживания потерь на валидации и остановки обучения, если потери не уменьшаются в течение 3 эпох.
⦁ Анализ результатов: После обучения постройте графики зависимости точности и потерь от эпохи, как для обучающей, так и для валидационной выборки. Подсказка: используйте history = model.fit(...) и затем history.history для доступа к значениям метрик во время обучения. Наличие переобучения можно определить, если точность на обучающей выборке продолжает расти, а на валидационной выборке начинает падать или выходит на плато. Переобучение также характеризуется большим разрывом между точностью на обучающей и валидационной выборках.


# Самостоятельная работа:  
1. Все представленные выше задания выполнить с помощью библиотеки PyTorch. 
2. Найдите набор данных с изображением жестов. Обучите разработанную модель на этом наборе данных (на 5 классах). Подберите оптимальную архитектуру модели. (TensorFlow и PyTorch) 