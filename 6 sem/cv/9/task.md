
# Задания в классе:

## Задание 1: Изменение архитектуры сети
Измените архитектуру CNN, варьируя следующие параметры:
•	Количество фильтров в сверточных слоях: Например, попробуйте 32, 64, 128 фильтров в каждом слое.
•	Размер фильтров: Исследуйте фильтры размером 3x3, 5x5 или 7x7.
•	Количество сверточных слоев: Добавьте или удалите сверточные слои.
•	Количество нейронов в полносвязных слоях: Измените количество нейронов в плотных слоях после сверточных (например, 64, 256, 512).
•	Количество полносвязных слоев: Добавьте или удалите полносвязные слои.
Обучите модель с каждой измененной архитектурой и сравните результаты с исходной моделью. Как изменение архитектуры влияет на точность и скорость обучения? Результаты задокументируйте в таблице, указав количество параметров модели.

## Задание 2: Эксперименты с функциями активации
Замените функцию активации ReLU в сверточных и полносвязных слоях на другие функции, такие как sigmoid, tanh, ELU, LeakyReLU. Обучите модель с каждой из этих функций и сравните результаты. Какие функции активации дают лучшую точность? Влияют ли разные функции активации на скорость обучения? Результаты задокументируйте в таблице.

## Задание 3: Оптимизаторы

Попробуйте использовать различные оптимизаторы, такие как SGD, RMSprop, Adagrad, AdamW вместо Adam. Настройте параметры оптимизаторов (например, learning rate для SGD, momentum, decay). Сравните результаты и объясните влияние различных оптимизаторов на процесс обучения и итоговую точность. Результаты задокументируйте в таблице.

## Задание 4: Оценка качества обучения и борьба с переобучением
•	Добавление Dropout: Добавьте слой Dropout после сверточных и/или полносвязных слоев с коэффициентом dropout от 0.2 до 0.5.
•	Изменение размера батча: Экспериментируйте с разными размерами батча (например, 16, 32, 64, 128). Как размер батча влияет на скорость обучения и точность?
•	Аугментация данных: Применяйте различные методы аугментации данных (поворот, масштабирование, сдвиг, отражение) для увеличения размера обучающей выборки и повышения обобщающей способности модели.
•	Early Stopping: Используйте callback EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True) для отслеживания потерь на валидации и остановки обучения, если потери не уменьшаются в течение 3 эпох, восстанавливая лучшие веса.
•	L1/L2 регуляризация: Добавьте L1 или L2 регуляризацию к весам сверточных и/или полносвязных слоев.
•	Анализ результатов: После обучения постройте графики зависимости точности и потерь от эпохи, как для обучающей, так и для валидационной выборки (используйте history = model.fit(...) и history.history). Наличие переобучения можно определить по расхождению графиков точности и потерь для обучающей и валидационной выборок.

# Самостоятельная работа:

1.	Все представленные выше задания выполнить с помощью библиотеки PyTorch.
2.	Найдите набор данных с изображением жестов. Обучите разработанную модель на этом наборе данных (на 5 классах). Подберите оптимальную архитектуру модели. (TensorFlow и PyTorch). Набор данных взять на одном из открытых ресурсов: https://www.kaggle.com/datasets; https://public.roboflow.com
